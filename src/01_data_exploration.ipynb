{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:06:29.627120900Z",
     "start_time": "2023-11-15T10:06:29.610425800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from dataclasses import dataclass\n",
    "import hvplot.pandas \n",
    "# hv.renderer('bokeh').theme = 'dark_minimal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "It is always a good advice to use standard formats to transfer data from one computer to another. One of the most used formats is parquet. The following code should help you to load the data into your python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:06:36.095981800Z",
     "start_time": "2023-11-15T10:06:35.378864500Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_root = Path(r\"../data\") # Raw string works without escaping \\\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Case():\n",
    "    info: pd.DataFrame\n",
    "    measurements: pd.DataFrame\n",
    "\n",
    "\n",
    "class RawDataset():\n",
    "\n",
    "    def __init__(self, root, unit = \"VG4\", load_training=False, load_synthetic=False) -> None:\n",
    "        \n",
    "        \n",
    "        read_pq_file = lambda f: pq.read_table(root / f).to_pandas()\n",
    "        \n",
    "        \n",
    "        cases = {\n",
    "            \"test\": [f\"{unit}_generator_data_testing_real_measurements.parquet\", root / f\"{unit}_generator_data_testing_real_info.csv\" ], \n",
    "        }\n",
    "        \n",
    "        if load_training:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"train\": [f\"{unit}_generator_data_training_measurements.parquet\", root / f\"{unit}_generator_data_training_info.csv\" ], \n",
    "            }\n",
    "        \n",
    "        if load_synthetic:\n",
    "            cases = {\n",
    "                **cases,\n",
    "                \"test_s01\": [f\"{unit}_generator_data_testing_synthetic_01_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_01_info.csv\"], \n",
    "                \"test_s02\": [f\"{unit}_generator_data_testing_synthetic_02_measurements.parquet\", root / f\"{unit}_generator_data_testing_synthetic_02_info.csv\"]\n",
    "            }\n",
    "        \n",
    "        \n",
    "        self.data_dict = dict()\n",
    "        \n",
    "        for id_c, c in cases.items():\n",
    "            # if you need to verify the parquet header:\n",
    "            # pq_rows = RawDataset.read_parquet_schema_df(root / c[0])\n",
    "            info = pd.read_csv(c[1])\n",
    "            measurements = read_pq_file(c[0])\n",
    "            self.data_dict[id_c] = Case(info, measurements)\n",
    "            \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def read_parquet_schema_df(uri: str) -> pd.DataFrame:\n",
    "        \"\"\"Return a Pandas dataframe corresponding to the schema of a local URI of a parquet file.\n",
    "\n",
    "        The returned dataframe has the columns: column, pa_dtype\n",
    "        \"\"\"\n",
    "        # Ref: https://stackoverflow.com/a/64288036/\n",
    "        schema = pq.read_schema(uri, memory_map=True)\n",
    "        schema = pd.DataFrame(({\"column\": name, \"pa_dtype\": str(pa_dtype)} for name, pa_dtype in zip(schema.names, schema.types)))\n",
    "        schema = schema.reindex(columns=[\"column\", \"pa_dtype\"], fill_value=pd.NA)  # Ensures columns in case the parquet file has an empty dataframe.\n",
    "        return schema\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "rds_u4 = RawDataset(dataset_root, \"VG4\", load_synthetic=False, load_training=True)\n",
    "rds_u5 = RawDataset(dataset_root, \"VG5\", load_synthetic=True, load_training=True)\n",
    "rds_u6 = RawDataset(dataset_root, \"VG6\", load_synthetic=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation Modes\n",
    "\n",
    "As you can see, the operation modes (at the very bottom) are not in the info schema, but you can find them when you explore the parquet schema (metadata of the parquet data file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:06:58.253155200Z",
     "start_time": "2023-11-15T10:06:58.214134100Z"
    }
   },
   "outputs": [],
   "source": [
    "RawDataset.read_parquet_schema_df(dataset_root / \"VG4_generator_data_testing_real_measurements.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `head()` to glance at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:06:59.990856400Z",
     "start_time": "2023-11-15T10:06:59.975483200Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"train\"].info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:01.323927Z",
     "start_time": "2023-11-15T10:07:01.274220800Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"train\"].measurements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "- Make sure that you explore the data in detail. For example, we will see that there is quite a big gap in operation from train to test if we look at the injector pressures of unit 4.\n",
    "- To not overload our machine when plotting, we can use an index selection trick to downsample the data (since the dataframes are quite large) `[::100]`.\n",
    "    - Please be aware that this is just to get some insights, we are only plotting every 100th step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:03.738962500Z",
     "start_time": "2023-11-15T10:07:03.713963800Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u5.data_dict[\"train\"].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite many signals, so it makes sense to study them in groups. Here we focus on the temperatures `_tmp`), but you are supposed to look at all the other signals as well of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:06.133926300Z",
     "start_time": "2023-11-15T10:07:06.089813800Z"
    }
   },
   "outputs": [],
   "source": [
    "df = rds_u4.data_dict[\"train\"].info\n",
    "temperature_attrs = df[df.attribute_name.str.endswith(\"_tmp\")]\n",
    "temperature_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas to directly plot the data. Usually, panda uses matplotlib, but when we install the hvplot module, we can have a much nicer html data explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"train\"].measurements[::100].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:08.571769400Z",
     "start_time": "2023-11-15T10:07:07.266240600Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"train\"].measurements[::100].hvplot.explorer(y=[v for v in temperature_attrs.attribute_name.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other example would be to look at injector pressures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:12.664033100Z",
     "start_time": "2023-11-15T10:07:12.646992300Z"
    }
   },
   "outputs": [],
   "source": [
    "df = rds_u4.data_dict[\"train\"].info\n",
    "temperature_attrs = df[df.attribute_name.str.contains(\"injector\")]\n",
    "temperature_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:14.885310400Z",
     "start_time": "2023-11-15T10:07:14.312679Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"train\"].measurements[::100].hvplot.explorer(y=[\"injector12_pressure\", \"injector34_pressure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the test set is quite short compared to the training set. For injector 1, the test distribution is different compared to the training distribution (distribution shift).\n",
    "Be aware that such distribution shifts can greatly influence the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:16.207768Z",
     "start_time": "2023-11-15T10:07:15.575112600Z"
    }
   },
   "outputs": [],
   "source": [
    "rds_u4.data_dict[\"test\"].measurements[::100].hvplot.explorer(y=[\"injector12_pressure\", \"injector34_pressure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the machines in operation next to each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:24.484320900Z",
     "start_time": "2023-11-15T10:07:19.275456800Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = (\n",
    "    rds_u4.data_dict[\"train\"].measurements[::1000].hvplot()\n",
    "    + rds_u5.data_dict[\"train\"].measurements[::1000].hvplot()\n",
    "    ).cols(1)\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the test datasets are recorded troughout different seasons, which can have an impact on the recorded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:07:29.257069500Z",
     "start_time": "2023-11-15T10:07:23.564972400Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = (\n",
    "    rds_u4.data_dict[\"test\"].measurements[::1000].hvplot()\n",
    "    + rds_u5.data_dict[\"test\"].measurements[::1000].hvplot()\n",
    "    + rds_u6.data_dict[\"test\"].measurements[::1000].hvplot()\n",
    "    ).cols(1)\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Try to explore the data as much as possible before engineering the solution. The main difficulty in real-world problems often isn't the complexity, but understanding the actual problem you are trying to solve, as there is no \"guidance\" like in the exercises you saw throughout this class. Remember, there are always trade-offs when selecting a model which you should evaluate before investing a lot of time on the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration of VG4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_u4 = RawDataset(dataset_root, \"VG4\", load_synthetic=False, load_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg4_train, vg4_test = rds_u4.data_dict['train'], rds_u4.data_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the structure of the measurements data\n",
    "print(\"VG4 Training Data Structure:\\n\")\n",
    "print(vg4_train.measurements.info())  # Overview of columns and types\n",
    "\n",
    "# Checking metadata for additional information on each variable\n",
    "print(\"\\nVG4 Training Metadata Structure:\\n\")\n",
    "print(vg4_train.info.head())  # Displaying metadata to see attributes for each signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the measurements data\n",
    "missing_data = vg4_train.measurements.isnull().sum()\n",
    "print(\"\\nMissing Data Summary:\\n\")\n",
    "print(missing_data[missing_data > 0])  # Only displaying columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring operating modes and their distributions\n",
    "operating_modes = ['turbine_mode', 'pump_mode', 'machine_on', 'machine_off', 'equilibrium_turbine_mode', 'equilibrium_pump_mode', 'dyn_only_on']\n",
    "mode_counts = vg4_train.measurements[operating_modes].sum()\n",
    "print(\"\\nOperating Modes Distribution:\\n\")\n",
    "print(mode_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify all columns in the measurements data\n",
    "all_columns = vg4_train.measurements.columns\n",
    "\n",
    "# Separate features into categories\n",
    "non_boolean_non_operating = []  # Stores numeric features that aren't booleans or operating conditions\n",
    "\n",
    "for col in all_columns:\n",
    "    if col not in operating_modes:\n",
    "        # Check if column values are boolean-like (True/False or binary 0/1)\n",
    "        unique_vals = vg4_train.measurements[col].dropna().unique()\n",
    "        if len(unique_vals) > 2 or not set(unique_vals).issubset({0, 1, True, False}):\n",
    "            non_boolean_non_operating.append(col)\n",
    "        else:\n",
    "            print('Non-unique or boolean feature:', col)\n",
    "\n",
    "print(\"Non-boolean, Non-operating Condition Features:\")\n",
    "print(non_boolean_non_operating)\n",
    "\n",
    "all_features = vg4_train.measurements[non_boolean_non_operating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix for the measurements data\n",
    "correlation_matrix = all_features.corr()\n",
    "\n",
    "# Plotting correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Correlation Matrix for VG4 Measurements\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = np.abs(correlation_matrix)\n",
    "\n",
    "# Convert correlation matrix to distance matrix for clustering (1 - correlation)\n",
    "distance_matrix = 1 - correlation_matrix\n",
    "\n",
    "# Perform hierarchical clustering using the distance matrix\n",
    "linked = linkage(squareform(distance_matrix), method='ward')\n",
    "\n",
    "# Create a dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, labels=correlation_matrix.index)\n",
    "plt.title('Dendrogram of Asset Clusters')\n",
    "plt.xlabel('Assets (ID)')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Clustered heatmap of the correlation matrix\n",
    "plt.figure(figsize=(40, 30))\n",
    "sns.clustermap(correlation_matrix, method='ward', cmap='coolwarm', annot=False)\n",
    "plt.title('Clustered Heatmap of Asset Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select only the numerical columns for scaling\n",
    "# We'll ignore boolean columns and any columns that aren't numerical\n",
    "numerical_columns = non_boolean_non_operating\n",
    "\n",
    "# Fit the scaler on the training set and transform it\n",
    "vg4_train_standardized = vg4_train.measurements.copy()  # Creating a copy to avoid modifying the original data\n",
    "vg4_train_standardized[numerical_columns] = scaler.fit_transform(vg4_train.measurements[numerical_columns])\n",
    "\n",
    "# Transform the test set using the same scaler (we do not refit to ensure consistency)\n",
    "vg4_test_standardized = vg4_test.measurements.copy()  # Copying test data\n",
    "vg4_test_standardized[numerical_columns] = scaler.transform(vg4_test.measurements[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important features to compare between train and test sets\n",
    "important_features = non_boolean_non_operating #[col for col in vg4_train.measurements.columns if 'tmp' in col]\n",
    "\n",
    "# Prepare a dictionary to store the plots for each feature\n",
    "comparison_plots = {}\n",
    "\n",
    "# Generate distribution plots for each feature in the list\n",
    "for feature in important_features:\n",
    "    # Check if feature exists in both train and test datasets\n",
    "    if feature in vg4_train_standardized.columns and feature in vg4_test_standardized.columns:\n",
    "        train_data = vg4_train_standardized[feature].dropna()\n",
    "        test_data = vg4_test_standardized[feature].dropna()\n",
    "\n",
    "        # Overlay normalized histograms for train and test distributions\n",
    "        comparison_plot = train_data.hvplot.hist(\n",
    "            bins=30, alpha=0.5, width=500, height=400, color='blue', \n",
    "            ylabel='Density', xlabel=f'{feature} Value', label='Train', \n",
    "            normed=True  # Normalize the histogram\n",
    "        ) * test_data.hvplot.hist(\n",
    "            bins=30, alpha=0.5, color='orange', label='Test', \n",
    "            normed=True  # Normalize the histogram\n",
    "        )\n",
    "\n",
    "        # Add title and legend\n",
    "        comparison_plot = comparison_plot.opts(\n",
    "            title=f'Normalized pdf of {feature}',\n",
    "            legend_position='top_right',\n",
    "            ylim=(0, 1)  # Set y-axis limit to 1\n",
    "        )\n",
    "        \n",
    "        # Store each plot in a dictionary for easy access and display\n",
    "        comparison_plots[feature] = comparison_plot\n",
    "\n",
    "# Display the plots as a grid layout\n",
    "hv.Layout(comparison_plots.values()).cols(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
